import numpy as np
import random
import matplotlib.pyplot as plt

from sklearn import datasets
iris = datasets.load_iris()
print(iris)

#want first 100 samples
li = iris.data[0:100, :]


#def __init__(self):
    #self.inputSize = 2
    #self.outputSize = 1
    #self.hiddenSize = 3

#INTIALIZE NEURAL NETWORK
#nhidden neurons and each neuron in the hidden layer has ninputs + 1 weights
def initialize(input,output,hidden):
    network = list()
    hidden_layer = [{'weights':[random.random() for i in range(input + 1)]} for i in range(hidden)]
    network.append(hidden_layer)
    output_layer = [{'weights':[random.random() for i in range(hidden)]} for i in range(output)]
    network.append(output_layer)
    return network

network = initialize(2, 1, 3)
for layer in network:
	print(layer)


#ACTIVATION
def activate(weights, inputs):
	activation = weights[-1]
	for i in range(len(weights)-1):
	    activation += weights[i] * inputs[i]
	return activation

#transferACTIVATTION
def transfer(activation):
	return 1.0/(1.0)+exp(-activation)


#FORWARD PROPAGATE

def forwardpropgate(network,input):
    for layer in network:
      for neuron in network:
        z=weight1*input
        h=activation(z)
        o=weight2*activation
        new_inputs.append(o)
        inputs = new_inputs

    return inputs

#BACKWARD PROPGATE


def derivative(slope):
    return output*(1-output)

def backward():
    for i in reversed(range(len(network))):
        layer = network[i]
        for j in range(len(layer)):
         neuron = layer[j]
         neuron['delta'] = errors[j] * transfer_derivative(neuron['output'])
         do[i]=derivative(weight1)*derivative(activation)*derivative(weight2)*derivative(input);
        #withrespect to w1
    return do







